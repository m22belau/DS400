---
title: "Normal Normal"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(bayesrules)
```

## [5.3 Normal-Normal conjugate family](https://www.bayesrulesbook.com/chapter-5#normal-normal-conjugate-family)

We now have two conjugate families in our toolkit: the Beta-Binomial and the Gamma-Poisson. But many more conjugate families exist! It’s impossible to cover them all, but there is a third conjugate family that’s especially helpful to know: the **Normal-Normal**.

Consider a data story. As scientists learn more about brain health, the dangers of concussions (hence of activities in which participants sustain repeated concussions) are gaining greater attention ([Bachynski 2019](https://www.bayesrulesbook.com/chapter-5#ref-bachynski2019no)). Among all people who have a history of concussions, we are interested in μ, the average volume (in cubic centimeters) of a specific part of the brain: the hippocampus. Though we don’t have prior information about this group in particular, Wikipedia tells us that among the general population of human adults, both halves of the hippocampus have a volume between 3.0 and 3.5 cubic centimeters.^[38](https://www.bayesrulesbook.com/chapter-5#fn38)^ Thus, the *total* hippocampal volume of *both* sides of the brain is between 6 and 7 cm3. Using this as a starting point, we’ll assume that the mean hippocampal volume among people with a history of concussions, μ, is also somewhere between 6 and 7 cm3, with an average of 6.5. We’ll balance this prior understanding with data on the hippocampal volumes of n=25 subjects, (Y1,Y2,…,Yn), using the **Normal-Normal Bayesian model**.

No matter the parameters, the Normal model is bell-shaped and symmetric around μ – thus as μ gets larger, the model shifts to the right along with it. Further, σ controls the variability of the Normal model – as σ gets larger, the model becomes more spread out.

Play with the parameter below

```{r}
plot_normal(mean = 10, sd = 1) +
  geom_vline(xintercept = 9, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = 11, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = 8, color = "coral") +
  geom_vline(xintercept = 12, color = "coral") +
  theme_minimal()
```

Using our understanding of a Normal model, we can now *tune* the prior hyperparameters θ and τ to reflect our prior understanding and uncertainty about the average hippocampal volume among people that have a history of concussions, μ. Based on our rigorous Wikipedia research that hippocampal volumes tend to be between 6 and 7 cm3, we’ll set the Normal prior mean θ to the midpoint, 6.5. Further, we’ll set the Normal prior standard deviation to τ=0.4. In other words, by [(5.12)](https://www.bayesrulesbook.com/chapter-5#eq:normal-scale), we think there’s a 95% chance that μ is somewhere between 5.7 and 7.3 cm3 (6.5±2∗0.4). This range is *wider*, and hence more conservative, than what Wikipedia indicated. Our uncertainty here reflects the fact that we didn’t vet the Wikipedia sources, we aren’t confident that the features for the typical adult translates to people with a history of concussions, and we generally aren’t sure what’s going on here (i.e., we’re not brain experts). Putting this together, our tuned prior model for μ is:

```{r}
plot_normal(mean = 6.5, sd = 0.4)
```

*Challenge*

-   Add 4 `geom_vline()` to the plot above to indicate 1 standard deviation capturing 68% of the data and 2 standard deviations capturing 95% of the data

```{r}
plot_normal(mean = 6.5, sd = 0.4) +
  geom_vline(xintercept = 6.1, linetype = "dashed", color = "blue") + # 6.5 - 0.4  (1 SD)
  geom_vline(xintercept = 6.9, linetype = "dashed", color = "blue") + # 6.5 + 0.4
  geom_vline(xintercept = 5.7, color = "coral") +  # 6.5 - 0.8  (2 SD)
  geom_vline(xintercept = 7.3, color = "coral") +  # 6.5 + 0.8
  theme_minimal()
```

Let’s apply and examine this result in our analysis of μ, the average hippocampal volume among people that have a history of concussions. We’ve already built our prior model of μ, μ∼N(6.5,0.42). Next, consider some data. The `football` data in **bayesrules**, a subset of the `FootballBrain` data in the **Lock5Data** package ([Lock et al. 2016](https://www.bayesrulesbook.com/chapter-5#ref-lock2016statistics)), includes results for a cross-sectional study of hippocampal volumes among 75 subjects ([Singh et al. 2014](https://www.bayesrulesbook.com/chapter-5#ref-singh2014relationship)): 25 collegiate football players with a history of concussions (`fb_concuss`), 25 collegiate football players that do not have a history of concussions (`fb_no_concuss`), and 25 control subjects. For our analysis, we’ll focus on the n=25 subjects with a history of concussions (`fb_concuss`):

```{r}
# Load the data
data(football)
```

*Challenge*

-   Visualize the football data in an interesting way utilizing `ggplot(()`

```{r}
ggplot(football, aes(x = years, y = volume, color = group)) +
  geom_point() #+
  #geom_smooth()
```

Back to our example

```{r}

concussion_subjects <- football %>%
  filter(group == "fb_concuss")
```

```{r}
concussion_subjects %>%
  summarize(mean(volume))
```

```{r}
ggplot(concussion_subjects, aes(x = volume)) + 
  geom_density()
```

```{r}
ggplot(concussion_subjects, aes(x = volume)) + 
  geom_histogram(bins = 5)
```

We plot this likelihood function using `plot_normal_likelihood()`, providing our observed `volume` data and data standard deviation σ=0.59 This likelihood illustrates the compatibility of our observed hippocampal data with different μ values. To this end, the hippocampal patterns observed in our data would most likely have arisen if the mean hippocampal volume across *all* people with a history of concussions, μ, were between 5.3 and 6.1 cm3. Further, we’re *most* likely to have observed a mean volume of ¯¯¯y= 5.735 among our 25 sample subjects if the underlying population mean μ were also 5.735.

```{r}
plot_normal_likelihood(y = concussion_subjects$volume, sigma = 0.59) 
```

It is important to note that we are not modeling the distribution of concussion patients hippocampal volume as we did above with `geom_density()`

-   It is important to note that we are plotting **distributions over μ (the population mean)**, not over individual data

-   likelihood_sd = σ/√n = 0.59/√25 = 0.12

Bringing all of these pieces together, we plot and summarize our Normal-Normal analysis of μ using `plot_normal_normal()` and `summarize_normal_normal()` in the **bayesrules** package. Though a compromise between the prior and data, our posterior understanding of μ is more heavily influenced by the latter. In light of our data, we are much more *certain* about the mean hippocampal volume among people with a history of concussions, and believe that this figure is somewhere in the range from 5.586 to 5.974 cm3 (5.78±2∗0.097).

```{r}
plot_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.59,
                   y_bar = 5.735, n = 25)
```

```{r}
summarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.59,
                   y_bar = 5.735, n = 25)
```

### Challenge

AI Prompting

-   Go back to your modeling scenarios that you created last class

-   Show your data, code, and outputs to an AI assistant

-   Ask it to critique your analysis

    -   Are there alternate/better bayesian methods/models to apply to the question your asking? Think of what we have covered (beta-binomial, gamma-poisson, normal-normal)

        -   I think a Beta–Binomial model would have been a more natural fit for our question (“How likely is a bus to arrive on time?”).

        -   ChatGPT pointed out that the Gamma–Poisson model assumes counts without a fixed denominator, while our data are structured as successes out of a known number of trials (`OnTime_Arrivals` out of `Total_Buses`). A Beta–Binomial directly models this setup. Another option could be a Normal–Normal model on the weekly proportions, but that would be more of an approximation.

        -   If we wanted to compare multiple routes at once, a hierarchical Beta–Binomial could also be powerful.

    -   What assumptions are associated with the model you chose? With those assumptions in mind, is the model still a good fit for your question/data?

        -   The Gamma–Poisson assumes that the number of on-time arrivals follows a Poisson distribution with some rate λ, and that λ itself follows a Gamma prior. This works mechanically but doesn’t fully match our data, since on-time arrivals are naturally capped by `Total_Buses`.

        -   The Beta–Binomial, on the other hand, models the number of on-time buses out of the total buses directly, which matches the way our data is actually structured. It respects the fact that you can’t have more on-time buses than total buses.

        -   So while the Gamma–Poisson is serviceable, I think the Beta–Binomial is a better fit for our dataset and question.

    -   Are there real datasets you can use to answer your question opposed to AI generated datasets?

        -   U.S. National Transit Database (NTD): <https://www.transit.dot.gov/ntd>

```{r}
library(here)

bus_ontime <- read_csv(here('data/hawaii_bus_ontime.csv'))
route1 <- bus_ontime[bus_ontime$Route == "Route 1", ]

```

```{r}
plot_beta(50, 50) 
# Prior: I think 50% of buses arrive on time (~35% — ~65%) — Somewhat confident
```

```{r}
sum(route1$OnTime_Arrivals)
sum(route1$Total_Buses)
```

```{r}
plot_beta_binomial(alpha = 50, beta = 50, y = 424, n = 469)
summarize_beta_binomial(alpha = 50, beta = 50, y = 424, n = 469)
```

After doing this, the Beta–Binomial model makes more sense for this data because it focuses on the **proportion of buses that arrive on time** rather than the raw counts.

-   Saying something like “I think 50% of buses will arrive on time” feels more intuitive than saying “I think 20 of the 40 buses will arrive on time,” especially since the number of buses per route or week can vary. This way, the model works with percentages, which are easier to interpret and more meaningful for comparing different weeks or routes.
